Column cat selection:
selected_cat_col = ['host_response_time', 'host_is_superhost',
    'host_identity_verified', 'is_location_exact', 'property_type', 'room_type',
    'bed_type', 'instant_bookable', 'cancellation_policy',
    'require_guest_profile_picture', 'host_has_profile_pic']

nominal_cols = nominal_columns = X_train_cat_filtered.columns.difference(ordinal_columns)

ordinal_cols = ['host_response_time', 'cancellation_policy']
### Functions ###

# numerical #

OK 1) Num droping columns

df_numerical = df_raw.select_dtypes(exclude="object")

# from describe()
num_features_to_drop = ['Unnamed: 0', 'scrape_id', 'availability_30', 'availability_60', 'availability_90',
 'availability_365', 'calculated_host_listings_count', 'minimum_maximum_nights',
 'maximum_maximum_nights', 'minimum_nights_avg_ntm','maximum_nights_avg_ntm',
 'calculated_host_listings_count_private_rooms',
 'calculated_host_listings_count_shared_rooms']

 X_train_num_tr.drop(columns=num_features_to_drop, inplace=True)

OK 2) num outlier in lon lat :

df_train_outliar = df_train[(df_train['latitude'] > 0) & (df_train['longitude'] > 0)]

OK 3) Feature creation :

df_train['beds_per_bedroom'] = df_train['beds'] / df_train['bedrooms']

OK 4) Num missing values :

# Calculate the percentage of null values (NaN or None) for all columns
nan_percentages = X_train.isnull().sum() / float(len(X_train))

# Sort the Series in descending order
nan_percentages_sorted = nan_percentages.sort_values(ascending=False)

# Set the threshold for maximum acceptable NaN percentage
threshold = 0.99

# Identify columns to keep (NaN percentage below the threshold)
columns_to_drop = nan_percentages_sorted[nan_percentages_sorted >= threshold].index

X_train = X_train.drop(columns_to_drop, axis=1)

# Replace infinite values with NaNs
X_train_num.replace([np.inf, -np.inf], np.nan, inplace=True)

# Replace values exceeding float64 limits with NaNs
X_train_num[X_train_num.abs() > np.finfo(np.float64).max] = np.nan

### Categorical ###

OK cat_outliers

X_train_cat_filtered = X_train_cat[(X_train_cat['host_is_superhost'] != '3') &
                                   (X_train_cat['is_location_exact'] != '2019-03-13') &
                                   (X_train_cat['room_type'] != '2019-03-06') &
                                   (X_train_cat['bed_type'] != '10.0') &
                                   (X_train_cat['host_has_profile_pic'] != '$501.00')]

Ordinal

from sklearn.preprocessing import OrdinalEncoder

# Define the desired order for the ordinal columns
response_time_order = ['within an hour', 'within a few hours', 'within a day', 'a few days or more', np.nan]
cancellation_policy_order = ['flexible', 'moderate', 'strict', 'strict_14_with_grace_period', 'super_strict_30', 'super_strict_60']

# Create and fit OrdinalEncoders for each column with their respective orders
response_time_encoder = OrdinalEncoder(categories=[response_time_order], handle_unknown='use_encoded_value', unknown_value=-1)
cancellation_policy_encoder = OrdinalEncoder(categories=[cancellation_policy_order], handle_unknown='use_encoded_value', unknown_value=-1)

# Select the ordinal columns to be encoded
ordinal_columns = ['host_response_time', 'cancellation_policy']

# Create a copy of the DataFrame to avoid modifying the original
X_train_cat_ord = X_train_cat_filtered[ordinal_columns].copy()

# Transform the columns in-place, replacing the original values
X_train_cat_ord['host_response_time'] = response_time_encoder.fit_transform(X_train_cat_ord[['host_response_time']])
X_train_cat_ord['cancellation_policy'] = cancellation_policy_encoder.fit_transform(X_train_cat_ord[['cancellation_policy']])

Grouping categories

# Calculate value counts and relative frequencies
value_counts = X_train_cat_filtered['property_type'].value_counts()
relative_freqs = value_counts / len(X_train_cat_filtered)

# Set a threshold (e.g., keep categories that occur at least 1% of the time)
threshold = 0.005

# Identify significant categories
significant_categories = relative_freqs[relative_freqs >= threshold].index

# Replace infrequent categories with 'Other'
X_train_cat_filtered['property_type'] = X_train_cat_filtered['property_type'].apply(lambda x: x if x in significant_categories else 'Other')
X_train_cat_filtered['property_type'].value_counts()

===

imputer_num

# 1. Median Imputation
median_imputer = SimpleImputer(strategy='median', fill_value=0)

# 2. Zero Imputation (for cross-validation)
zero_imputer = SimpleImputer(strategy='constant', fill_value=0)

num_scaler

scaler = MinMaxScaler()  # Create the scaler object

ord_encoder

1111

nom_encoder

1111
